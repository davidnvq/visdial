{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from visdial.encoders import Encoder\n",
    "from visdial.decoders import Decoder\n",
    "from visdial.model import EncoderDecoderModel\n",
    "from visdial.data.dataset import VisDialDataset, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'img_norm': 1,\n",
       "  'concat_history': True,\n",
       "  'max_sequence_length': 20,\n",
       "  'vocab_min_count': 5},\n",
       " 'model': {'encoder': 'lf',\n",
       "  'decoder': 'disc',\n",
       "  'img_feature_size': 2048,\n",
       "  'word_embedding_size': 300,\n",
       "  'lstm_hidden_size': 512,\n",
       "  'lstm_num_layers': 2,\n",
       "  'dropout': 0.2},\n",
       " 'solver': {'training_splits': 'train'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_yml = 'configs/lf_disc_faster_rcnn_x101.yml'\n",
    "config = yaml.load(open(config_yml), Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary('/Users/quanguet/datasets/visdial/visdial_1.0_word_counts_train.json')\n",
    "# Pass vocabulary to construct Embedding layer.\n",
    "encoder = Encoder(config[\"model\"], vocab)\n",
    "decoder = Decoder(config[\"model\"], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EncoderDecoderModel(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth\n"
     ]
    }
   ],
   "source": [
    "from visdial.utils.checkpointing import load_checkpoint\n",
    "net = load_checkpoint('/Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth', net, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.word_embed.weight torch.Size([11322, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.hist_linear.weight torch.Size([512, 1024])\n",
      "encoder.hist_linear.bias torch.Size([512])\n",
      "encoder.ques_linear.weight torch.Size([512, 1024])\n",
      "encoder.ques_linear.bias torch.Size([512])\n",
      "encoder.image_features_projection.weight torch.Size([512, 2048])\n",
      "encoder.image_features_projection.bias torch.Size([512])\n",
      "encoder.attention_proj.weight torch.Size([1, 512])\n",
      "encoder.attention_proj.bias torch.Size([1])\n",
      "encoder.fusion.weight torch.Size([512, 3072])\n",
      "encoder.fusion.bias torch.Size([512])\n",
      "decoder.word_embed.weight torch.Size([11322, 300])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "decoder.option_linear.weight torch.Size([512, 1024])\n",
      "decoder.option_linear.bias torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for p in net.named_parameters():\n",
    "    print(p[0], p[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ndict = torch.load('/Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndict = ndict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.word_embed.weight torch.Size([11322, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.hist_linear.weight torch.Size([512, 1024])\n",
      "encoder.hist_linear.bias torch.Size([512])\n",
      "encoder.ques_linear.weight torch.Size([512, 1024])\n",
      "encoder.ques_linear.bias torch.Size([512])\n",
      "encoder.image_features_projection.weight torch.Size([512, 2048])\n",
      "encoder.image_features_projection.bias torch.Size([512])\n",
      "encoder.attention_proj.weight torch.Size([1, 512])\n",
      "encoder.attention_proj.bias torch.Size([1])\n",
      "encoder.fusion.weight torch.Size([512, 3072])\n",
      "encoder.fusion.bias torch.Size([512])\n",
      "decoder.word_embed.weight torch.Size([11322, 300])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "decoder.option_linear.weight torch.Size([512, 1024])\n",
      "decoder.option_linear.bias torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for k in ndict:\n",
    "    print(k, ndict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
