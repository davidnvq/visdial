{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from visdial.encoders import Encoder\n",
    "from visdial.decoders import Decoder\n",
    "from visdial.model import EncoderDecoderModel\n",
    "from visdial.data.dataset import VisDialDataset, Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'img_norm': 1,\n",
       "  'concat_history': True,\n",
       "  'max_sequence_length': 20,\n",
       "  'vocab_min_count': 5},\n",
       " 'model': {'encoder': 'lf',\n",
       "  'decoder': 'disc',\n",
       "  'img_feature_size': 2048,\n",
       "  'word_embedding_size': 300,\n",
       "  'lstm_hidden_size': 512,\n",
       "  'lstm_num_layers': 2,\n",
       "  'dropout': 0.2},\n",
       " 'solver': {'training_splits': 'train'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_yml = 'configs/lf_disc_faster_rcnn_x101.yml'\n",
    "config = yaml.load(open(config_yml), Loader=yaml.FullLoader)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary('/Users/quanguet/datasets/visdial/visdial_1.0_word_counts_train.json')\n",
    "# Pass vocabulary to construct Embedding layer.\n",
    "encoder = Encoder(config[\"model\"], vocab)\n",
    "decoder = Decoder(config[\"model\"], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EncoderDecoderModel(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EncoderDecoderModel:\n\tsize mismatch for encoder.hist_rnn.rnn_model.weight_ih_l1: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 1024]).\n\tsize mismatch for encoder.ques_rnn.rnn_model.weight_ih_l1: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 1024]).\n\tsize mismatch for decoder.option_rnn.rnn_model.weight_ih_l1: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4b244b18534a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisdial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpointing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/repos/visdial/visdial/utils/checkpointing.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint_pthpath, model, optimizer, device, resume)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/repos/visdial/visdial/utils/checkpointing.py\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(net, pretrained_dict)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mmodel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderDecoderModel:\n\tsize mismatch for encoder.hist_rnn.rnn_model.weight_ih_l1: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 1024]).\n\tsize mismatch for encoder.ques_rnn.rnn_model.weight_ih_l1: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 1024]).\n\tsize mismatch for decoder.option_rnn.rnn_model.weight_ih_l1: copying a param with shape torch.Size([2048, 2048]) from checkpoint, the shape in current model is torch.Size([2048, 1024])."
     ]
    }
   ],
   "source": [
    "from visdial.utils.checkpointing import load_checkpoint\n",
    "net = load_checkpoint('/Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth', net, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.word_embed.weight torch.Size([11322, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.hist_linear.weight torch.Size([512, 1024])\n",
      "encoder.hist_linear.bias torch.Size([512])\n",
      "encoder.ques_linear.weight torch.Size([512, 1024])\n",
      "encoder.ques_linear.bias torch.Size([512])\n",
      "encoder.image_features_projection.weight torch.Size([512, 2048])\n",
      "encoder.image_features_projection.bias torch.Size([512])\n",
      "encoder.attention_proj.weight torch.Size([1, 512])\n",
      "encoder.attention_proj.bias torch.Size([1])\n",
      "encoder.fusion.weight torch.Size([512, 3072])\n",
      "encoder.fusion.bias torch.Size([512])\n",
      "decoder.word_embed.weight torch.Size([11322, 300])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "decoder.option_linear.weight torch.Size([512, 1024])\n",
      "decoder.option_linear.bias torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for p in net.named_parameters():\n",
    "    print(p[0], p[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ndict = torch.load('/Users/quanguet/checkpoints/lf_disc/may13/checkpoint_best_ndcg.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndict = ndict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.word_embed.weight torch.Size([11322, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.hist_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.hist_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.hist_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "encoder.ques_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "encoder.ques_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "encoder.ques_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "encoder.hist_linear.weight torch.Size([512, 1024])\n",
      "encoder.hist_linear.bias torch.Size([512])\n",
      "encoder.ques_linear.weight torch.Size([512, 1024])\n",
      "encoder.ques_linear.bias torch.Size([512])\n",
      "encoder.image_features_projection.weight torch.Size([512, 2048])\n",
      "encoder.image_features_projection.bias torch.Size([512])\n",
      "encoder.attention_proj.weight torch.Size([1, 512])\n",
      "encoder.attention_proj.bias torch.Size([1])\n",
      "encoder.fusion.weight torch.Size([512, 3072])\n",
      "encoder.fusion.bias torch.Size([512])\n",
      "decoder.word_embed.weight torch.Size([11322, 300])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0 torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l0_reverse torch.Size([2048, 300])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l0_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1 torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1 torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1 torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.weight_ih_l1_reverse torch.Size([2048, 1024])\n",
      "decoder.option_rnn.rnn_model.weight_hh_l1_reverse torch.Size([2048, 512])\n",
      "decoder.option_rnn.rnn_model.bias_ih_l1_reverse torch.Size([2048])\n",
      "decoder.option_rnn.rnn_model.bias_hh_l1_reverse torch.Size([2048])\n",
      "decoder.option_linear.weight torch.Size([512, 1024])\n",
      "decoder.option_linear.bias torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for k in ndict:\n",
    "    print(k, ndict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
