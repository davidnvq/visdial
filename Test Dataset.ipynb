{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from visdial.data.readers import (\n",
    "    DialogsReader,\n",
    "    DenseAnnotationsReader,\n",
    "    ImageFeaturesHdfReader,\n",
    ")\n",
    "from visdial.data.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisDialDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict[str, Any],\n",
    "        dialogs_jsonpath: str,\n",
    "        dense_annotations_jsonpath: Optional[str] = None,\n",
    "        overfit: bool = False,\n",
    "        in_memory: bool = False,\n",
    "        return_options: bool = True,\n",
    "        add_boundary_toks: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.return_options = return_options\n",
    "        self.add_boundary_toks = add_boundary_toks\n",
    "        self.dialogs_reader = DialogsReader(dialogs_jsonpath)\n",
    "\n",
    "        if \"val\" in self.split and dense_annotations_jsonpath is not None:\n",
    "            self.annotations_reader = DenseAnnotationsReader(\n",
    "                dense_annotations_jsonpath\n",
    "            )\n",
    "        else:\n",
    "            self.annotations_reader = None\n",
    "\n",
    "        self.vocabulary = Vocabulary(\n",
    "            config[\"word_counts_json\"], min_count=config[\"vocab_min_count\"]\n",
    "        )\n",
    "\n",
    "        # Initialize image features reader according to split.\n",
    "        image_features_hdfpath = config[\"image_features_train_h5\"]\n",
    "        if \"val\" in self.dialogs_reader.split:\n",
    "            image_features_hdfpath = config[\"image_features_val_h5\"]\n",
    "        elif \"test\" in self.dialogs_reader.split:\n",
    "            image_features_hdfpath = config[\"image_features_test_h5\"]\n",
    "\n",
    "        self.hdf_reader = ImageFeaturesHdfReader(\n",
    "            image_features_hdfpath, in_memory\n",
    "        )\n",
    "\n",
    "        # Keep a list of image_ids as primary keys to access data.\n",
    "        self.image_ids = list(self.dialogs_reader.dialogs.keys())\n",
    "        if overfit:\n",
    "            self.image_ids = self.image_ids[:5]\n",
    "\n",
    "    @property\n",
    "    def split(self):\n",
    "        return self.dialogs_reader.split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image_id, which serves as a primary key for current instance.\n",
    "        image_id = self.image_ids[index]\n",
    "\n",
    "        # Get image features for this image_id using hdf reader.\n",
    "        image_features = self.hdf_reader[image_id]\n",
    "        image_features = torch.tensor(image_features)\n",
    "        # Normalize image features at zero-th dimension (since there's no batch\n",
    "        # dimension).\n",
    "        if self.config[\"img_norm\"]:\n",
    "            image_features = normalize(image_features, dim=0, p=2)\n",
    "\n",
    "        # Retrieve instance for this image_id using json reader.\n",
    "        visdial_instance = self.dialogs_reader[image_id]\n",
    "        caption = visdial_instance[\"caption\"]\n",
    "        dialog = visdial_instance[\"dialog\"]\n",
    "\n",
    "        # Convert word tokens of caption, question, answer and answer options\n",
    "        # to integers.\n",
    "        caption = self.vocabulary.to_indices(caption)\n",
    "        for i in range(len(dialog)):\n",
    "            dialog[i][\"question\"] = self.vocabulary.to_indices(\n",
    "                dialog[i][\"question\"]\n",
    "            )\n",
    "            if self.add_boundary_toks:\n",
    "                dialog[i][\"answer\"] = self.vocabulary.to_indices(\n",
    "                    [self.vocabulary.SOS_TOKEN]\n",
    "                    + dialog[i][\"answer\"]\n",
    "                    + [self.vocabulary.EOS_TOKEN]\n",
    "                )\n",
    "            else:\n",
    "                dialog[i][\"answer\"] = self.vocabulary.to_indices(\n",
    "                    dialog[i][\"answer\"]\n",
    "                )\n",
    "\n",
    "            if self.return_options:\n",
    "                for j in range(len(dialog[i][\"answer_options\"])):\n",
    "                    if self.add_boundary_toks:\n",
    "                        dialog[i][\"answer_options\"][\n",
    "                            j\n",
    "                        ] = self.vocabulary.to_indices(\n",
    "                            [self.vocabulary.SOS_TOKEN]\n",
    "                            + dialog[i][\"answer_options\"][j]\n",
    "                            + [self.vocabulary.EOS_TOKEN]\n",
    "                        )\n",
    "                    else:\n",
    "                        dialog[i][\"answer_options\"][\n",
    "                            j\n",
    "                        ] = self.vocabulary.to_indices(\n",
    "                            dialog[i][\"answer_options\"][j]\n",
    "                        )\n",
    "\n",
    "        questions, question_lengths = self._pad_sequences(\n",
    "            [dialog_round[\"question\"] for dialog_round in dialog]\n",
    "        )\n",
    "        history, history_lengths = self._get_history(\n",
    "            caption,\n",
    "            [dialog_round[\"question\"] for dialog_round in dialog],\n",
    "            [dialog_round[\"answer\"] for dialog_round in dialog],\n",
    "        )\n",
    "        answers_in, answer_lengths = self._pad_sequences(\n",
    "            [dialog_round[\"answer\"][:-1] for dialog_round in dialog]\n",
    "        )\n",
    "        answers_out, _ = self._pad_sequences(\n",
    "            [dialog_round[\"answer\"][1:] for dialog_round in dialog]\n",
    "        )\n",
    "\n",
    "        # Collect everything as tensors for ``collate_fn`` of dataloader to\n",
    "        # work seamlessly questions, history, etc. are converted to\n",
    "        # LongTensors, for nn.Embedding input.\n",
    "        item = {}\n",
    "        item[\"img_ids\"] = torch.tensor(image_id).long()\n",
    "        item[\"img_feat\"] = image_features\n",
    "        item[\"ques\"] = questions.long()\n",
    "        item[\"hist\"] = history.long()\n",
    "        item[\"ans_in\"] = answers_in.long()\n",
    "        item[\"ans_out\"] = answers_out.long()\n",
    "        item[\"ques_len\"] = torch.tensor(question_lengths).long()\n",
    "        item[\"hist_len\"] = torch.tensor(history_lengths).long()\n",
    "        item[\"ans_len\"] = torch.tensor(answer_lengths).long()\n",
    "        item[\"num_rounds\"] = torch.tensor(\n",
    "            visdial_instance[\"num_rounds\"]\n",
    "        ).long()\n",
    "\n",
    "        if self.return_options:\n",
    "            if self.add_boundary_toks:\n",
    "                answer_options_in, answer_options_out = [], []\n",
    "                answer_option_lengths = []\n",
    "                for dialog_round in dialog:\n",
    "                    options, option_lengths = self._pad_sequences(\n",
    "                        [\n",
    "                            option[:-1]\n",
    "                            for option in dialog_round[\"answer_options\"]\n",
    "                        ]\n",
    "                    )\n",
    "                    answer_options_in.append(options)\n",
    "\n",
    "                    options, _ = self._pad_sequences(\n",
    "                        [\n",
    "                            option[1:]\n",
    "                            for option in dialog_round[\"answer_options\"]\n",
    "                        ]\n",
    "                    )\n",
    "                    answer_options_out.append(options)\n",
    "\n",
    "                    answer_option_lengths.append(option_lengths)\n",
    "                answer_options_in = torch.stack(answer_options_in, 0)\n",
    "                answer_options_out = torch.stack(answer_options_out, 0)\n",
    "\n",
    "                item[\"opt_in\"] = answer_options_in.long()\n",
    "                item[\"opt_out\"] = answer_options_out.long()\n",
    "                item[\"opt_len\"] = torch.tensor(answer_option_lengths).long()\n",
    "            else:\n",
    "                answer_options = []\n",
    "                answer_option_lengths = []\n",
    "                for dialog_round in dialog:\n",
    "                    options, option_lengths = self._pad_sequences(\n",
    "                        dialog_round[\"answer_options\"]\n",
    "                    )\n",
    "                    answer_options.append(options)\n",
    "                    answer_option_lengths.append(option_lengths)\n",
    "                answer_options = torch.stack(answer_options, 0)\n",
    "\n",
    "                item[\"opt\"] = answer_options.long()\n",
    "                item[\"opt_len\"] = torch.tensor(answer_option_lengths).long()\n",
    "\n",
    "            if \"test\" not in self.split:\n",
    "                answer_indices = [\n",
    "                    dialog_round[\"gt_index\"] for dialog_round in dialog\n",
    "                ]\n",
    "                item[\"ans_ind\"] = torch.tensor(answer_indices).long()\n",
    "\n",
    "        # Gather dense annotations.\n",
    "        if self.annotations_reader is not None:\n",
    "            dense_annotations = self.annotations_reader[image_id]\n",
    "            item[\"gt_relevance\"] = torch.tensor(\n",
    "                dense_annotations[\"gt_relevance\"]\n",
    "            ).float()\n",
    "            item[\"round_id\"] = torch.tensor(\n",
    "                dense_annotations[\"round_id\"]\n",
    "            ).long()\n",
    "\n",
    "        return item\n",
    "\n",
    "    def _pad_sequences(self, sequences: List[List[int]]):\n",
    "        \"\"\"Given tokenized sequences (either questions, answers or answer\n",
    "        options, tokenized in ``__getitem__``), padding them to maximum\n",
    "        specified sequence length. Return as a tensor of size\n",
    "        ``(*, max_sequence_length)``.\n",
    "\n",
    "        This method is only called in ``__getitem__``, chunked out separately\n",
    "        for readability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequences : List[List[int]]\n",
    "            List of tokenized sequences, each sequence is typically a\n",
    "            List[int].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, torch.Tensor\n",
    "            Tensor of sequences padded to max length, and length of sequences\n",
    "            before padding.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            sequences[i] = sequences[i][\n",
    "                : self.config[\"max_sequence_length\"] - 1\n",
    "            ]\n",
    "        sequence_lengths = [len(sequence) for sequence in sequences]\n",
    "\n",
    "        # Pad all sequences to max_sequence_length.\n",
    "        maxpadded_sequences = torch.full(\n",
    "            (len(sequences), self.config[\"max_sequence_length\"]),\n",
    "            fill_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        padded_sequences = pad_sequence(\n",
    "            [torch.tensor(sequence) for sequence in sequences],\n",
    "            batch_first=True,\n",
    "            padding_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        maxpadded_sequences[:, : padded_sequences.size(1)] = padded_sequences\n",
    "        return maxpadded_sequences, sequence_lengths\n",
    "\n",
    "    def _get_history(\n",
    "        self,\n",
    "        caption: List[int],\n",
    "        questions: List[List[int]],\n",
    "        answers: List[List[int]],\n",
    "    ):\n",
    "        # Allow double length of caption, equivalent to a concatenated QA pair.\n",
    "        caption = caption[: self.config[\"max_sequence_length\"] * 2 - 1]\n",
    "\n",
    "        for i in range(len(questions)):\n",
    "            questions[i] = questions[i][\n",
    "                : self.config[\"max_sequence_length\"] - 1\n",
    "            ]\n",
    "\n",
    "        for i in range(len(answers)):\n",
    "            answers[i] = answers[i][: self.config[\"max_sequence_length\"] - 1]\n",
    "\n",
    "        # History for first round is caption, else concatenated QA pair of\n",
    "        # previous round.\n",
    "        history = []\n",
    "        history.append(caption)\n",
    "        for question, answer in zip(questions, answers):\n",
    "            history.append(question + answer + [self.vocabulary.EOS_INDEX])\n",
    "        # Drop last entry from history (there's no eleventh question).\n",
    "        history = history[:-1]\n",
    "        max_history_length = self.config[\"max_sequence_length\"] * 2\n",
    "\n",
    "        if self.config.get(\"concat_history\", False):\n",
    "            # Concatenated_history has similar structure as history, except it\n",
    "            # contains concatenated QA pairs from previous rounds.\n",
    "            concatenated_history = []\n",
    "            concatenated_history.append(caption)\n",
    "            for i in range(1, len(history)):\n",
    "                concatenated_history.append([])\n",
    "                for j in range(i + 1):\n",
    "                    concatenated_history[i].extend(history[j])\n",
    "\n",
    "            max_history_length = (\n",
    "                self.config[\"max_sequence_length\"] * 2 * len(history)\n",
    "            )\n",
    "            history = concatenated_history\n",
    "\n",
    "        history_lengths = [len(round_history) for round_history in history]\n",
    "        maxpadded_history = torch.full(\n",
    "            (len(history), max_history_length),\n",
    "            fill_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        padded_history = pad_sequence(\n",
    "            [torch.tensor(round_history) for round_history in history],\n",
    "            batch_first=True,\n",
    "            padding_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        maxpadded_history[:, : padded_history.size(1)] = padded_history\n",
    "        return maxpadded_history, history_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config-yml CONFIG_YML]\n",
      "                             [--train-json TRAIN_JSON] [--val-json VAL_JSON]\n",
      "                             [--val-dense-json VAL_DENSE_JSON]\n",
      "                             [--gpu-ids GPU_IDS [GPU_IDS ...]]\n",
      "                             [--cpu-workers CPU_WORKERS] [--overfit]\n",
      "                             [--validate] [--in-memory]\n",
      "                             [--save-dirpath SAVE_DIRPATH]\n",
      "                             [--load-pthpath LOAD_PTHPATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-b7158e4d-e834-47b2-8f42-94bdf17521e6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from bisect import bisect\n",
    "\n",
    "from visdial.data.dataset import VisDialDataset\n",
    "from visdial.encoders import Encoder\n",
    "from visdial.decoders import Decoder\n",
    "from visdial.metrics import SparseGTMetrics, NDCG\n",
    "from visdial.model import EncoderDecoderModel\n",
    "from visdial.utils.checkpointing import CheckpointManager, load_checkpoint\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "        \"--config-yml\",\n",
    "        default=\"configs/lf_gen_faster_rcnn_x101.yml\",\n",
    "        help=\"Path to a config file listing reader, model and solver parameters.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--train-json\",\n",
    "        default=\"/home/ubuntu/datasets/myvisdial/data/visdial_1.0_train.json\",\n",
    "        help=\"Path to json file containing VisDial v1.0 training data.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--val-json\",\n",
    "        default=\"/home/ubuntu/datasets/myvisdial/data/visdial_1.0_val.json\",\n",
    "        help=\"Path to json file containing VisDial v1.0 validation data.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--val-dense-json\",\n",
    "        default=\"/home/ubuntu/datasets/myvisdial/data/visdial_1.0_val_dense_annotations.json\",\n",
    "        help=\"Path to json file containing VisDial v1.0 validation dense ground \"\n",
    "             \"truth annotations.\",\n",
    "        )\n",
    "\n",
    "parser.add_argument_group(\n",
    "        \"Arguments independent of experiment reproducibility\"\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--gpu-ids\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"List of ids of GPUs to use.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--cpu-workers\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of CPU workers for dataloader.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--overfit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Overfit model on 5 examples, meant for debugging.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--validate\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to validate on val split after every epoch.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--in-memory\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load the whole dataset and pre-extracted image features in memory. \"\n",
    "             \"Use only in presence of large RAM, atleast few tens of GBs.\",\n",
    "        )\n",
    "\n",
    "parser.add_argument_group(\"Checkpointing related arguments\")\n",
    "parser.add_argument(\n",
    "        \"--save-dirpath\",\n",
    "        default=\"/home/ubuntu/datasets/myvisdial/checkpoints/\",\n",
    "        help=\"Path of directory to create checkpoint directory and save \"\n",
    "             \"checkpoints.\",\n",
    "        )\n",
    "parser.add_argument(\n",
    "        \"--load-pthpath\",\n",
    "        default=\"\",\n",
    "        help=\"To continue training, path to .pth file of saved checkpoint.\",\n",
    "        )\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  concat_history: true\n",
      "  image_features_test_h5: /home/ubuntu/datasets/myvisdial/data/features_faster_rcnn_x101_test.h5\n",
      "  image_features_train_h5: /home/ubuntu/datasets/myvisdial/data/features_faster_rcnn_x101_train.h5\n",
      "  image_features_val_h5: /home/ubuntu/datasets/myvisdial/data/features_faster_rcnn_x101_val.h5\n",
      "  img_norm: 1\n",
      "  max_sequence_length: 20\n",
      "  vocab_min_count: 5\n",
      "  word_counts_json: /home/ubuntu/datasets/myvisdial/data/visdial_1.0_word_counts_train.json\n",
      "model:\n",
      "  decoder: gen\n",
      "  dropout: 0.2\n",
      "  encoder: lf\n",
      "  img_feature_size: 2048\n",
      "  lstm_hidden_size: 512\n",
      "  lstm_num_layers: 2\n",
      "  word_embedding_size: 300\n",
      "solver:\n",
      "  batch_size: 32\n",
      "  initial_lr: 1e-3\n",
      "  num_epochs: 100\n",
      "  training_splits: train\n",
      "\n",
      "config_yml          : configs/lf_gen_faster_rcnn_x101.yml\n",
      "train_json          : /home/ubuntu/datasets/myvisdial/data/visdial_1.0_train.json\n",
      "val_json            : /home/ubuntu/datasets/myvisdial/data/visdial_1.0_val.json\n",
      "val_dense_json      : /home/ubuntu/datasets/myvisdial/data/visdial_1.0_val_dense_annotations.json\n",
      "gpu_ids             : [1]\n",
      "cpu_workers         : 4\n",
      "overfit             : False\n",
      "validate            : False\n",
      "in_memory           : False\n",
      "save_dirpath        : /home/ubuntu/datasets/myvisdial/checkpoints/\n",
      "load_pthpath        : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 903/376083 [00:00<00:45, 8252.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376083/376083 [00:22<00:00, 16661.68it/s]\n",
      "  1%|          | 3408/337528 [00:00<00:19, 17061.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337528/337528 [00:21<00:00, 15431.05it/s]\n",
      "  1%|          | 1320/123287 [00:00<00:09, 13197.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [00:08<00:00, 15003.22it/s]\n",
      "  0%|          | 1271/376083 [00:00<00:29, 12708.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376083/376083 [00:22<00:00, 16601.61it/s]\n",
      "  0%|          | 1671/337528 [00:00<00:20, 16708.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337528/337528 [00:24<00:00, 14032.56it/s]\n",
      "  1%|          | 1314/123287 [00:00<00:09, 13136.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [00:08<00:00, 15057.62it/s]\n"
     ]
    }
   ],
   "source": [
    "run train_experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_ids\n",
      "img_feat\n",
      "ques\n",
      "hist\n",
      "ans_in\n",
      "ans_out\n",
      "ques_len\n",
      "hist_len\n",
      "ans_len\n",
      "num_rounds\n",
      "opt_in\n",
      "opt_out\n",
      "opt_len\n",
      "ans_ind\n"
     ]
    }
   ],
   "source": [
    "elem = vd[0]\n",
    "for key in elem:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vd.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem['opt_in'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100, 20])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem['opt_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 82,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "tensor([82,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(elem['opt_in'][1, 3, :])\n",
    "print(elem['opt_out'][1, 3, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'green </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(vocab.to_words((elem['opt_out'][1, 3, :].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<S> green <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(vocab.to_words((elem['opt_in'][1, 3, :].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elem = tr[0]\n",
    "elem['ans_in'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> adult <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> male <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> inside <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> yes , but there is a blanket in between them and the floor <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> it is tile <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> red and white <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> orange red <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> boxer <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> yes <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<S> tan <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(' '.join(vocab.to_words(elem['ans_in'][i].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "male </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "inside </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "yes , but there is a blanket in between them and the floor </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "it is tile </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "red and white </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "orange red </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "boxer </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "yes </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "tan </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(' '.join(vocab.to_words(elem['ans_out'][i].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = vd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([74, 12, 30, 54, 22, 24, 33, 54, 70, 43])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vel['ans_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
