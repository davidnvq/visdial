{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME_PATH /home/quanguet\n",
      "DATA_PATH /home/quanguet/datasets/visdial\n",
      "{\n",
      "    \"seed\": 0,\n",
      "    \"callbacks\": {\n",
      "        \"validate\": true,\n",
      "        \"resume\": false,\n",
      "        \"comet_project\": \"lf-bert-disc\",\n",
      "        \"path_pretrained_ckpt\": \"\",\n",
      "        \"path_dir_save_ckpt\": \"/home/quanguet/checkpoints/visdial/lf_disc/lf_bert_disc\"\n",
      "    },\n",
      "    \"dataset\": {\n",
      "        \"overfit\": true,\n",
      "        \"img_norm\": 1,\n",
      "        \"concat_history\": true,\n",
      "        \"batch_size\": 2,\n",
      "        \"cpu_workers\": 4,\n",
      "        \"max_seq_len\": 25,\n",
      "        \"is_return_options\": true,\n",
      "        \"is_add_boundaries\": true,\n",
      "        \"train\": {\n",
      "            \"path_feat_img\": \"/home/quanguet/datasets/visdial/features_faster_rcnn_x101_val.h5\",\n",
      "            \"path_json_dialogs\": \"/home/quanguet/datasets/visdial/visdial_1.0_val.json\",\n",
      "            \"path_feat_history\": \"/home/quanguet/datasets/visdial/features_bert_train_history.h5\",\n",
      "            \"path_feat_answers\": \"/home/quanguet/datasets/visdial/features_bert_train_answers.h5\",\n",
      "            \"path_feat_questions\": \"/home/quanguet/datasets/visdial/features_bert_train_questions.h5\",\n",
      "            \"path_json_dense_dialogs\": \"/home/quanguet/datasets/visdial/\",\n",
      "            \"path_json_word_count\": \"/home/quanguet/datasets/visdial/visdial_1.0_word_counts_train.json\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"path_feat_img\": \"/home/quanguet/datasets/visdial/features_faster_rcnn_x101_val.h5\",\n",
      "            \"path_json_dialogs\": \"/home/quanguet/datasets/visdial/visdial_1.0_val.json\",\n",
      "            \"path_feat_history\": \"/home/quanguet/datasets/visdial/\",\n",
      "            \"path_feat_answers\": \"/home/quanguet/datasets/visdial/\",\n",
      "            \"path_feat_questions\": \"/home/quanguet/datasets/visdial/\",\n",
      "            \"path_json_dense_dialogs\": \"/home/quanguet/datasets/visdial/visdial_1.0_val_dense_annotations.json\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"path_feat_img\": \"/home/quanguet/datasets/visdial/features_faster_rcnn_x101_test.h5\",\n",
      "            \"path_feat_history\": \"/home/quanguet/datasets/visdial/features_bert_test_dialogs.h5\",\n",
      "            \"path_feat_answers\": \"/home/quanguet/datasets/visdial/features_bert_test_answers.h5\",\n",
      "            \"path_feat_questions\": \"/home/quanguet/datasets/visdial/features_bert_test_questions.h5\",\n",
      "            \"path_json_dense_dialogs\": \"/home/quanguet/datasets/visdial/\"\n",
      "        }\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"tokenizer\": \"nlp\",\n",
      "        \"hidden_size\": 512,\n",
      "        \"dropout\": 0.2,\n",
      "        \"vocab_size\": 11322,\n",
      "        \"text_embeddings\": {\n",
      "            \"vocab_size\": 11322,\n",
      "            \"embedding_size\": 300,\n",
      "            \"hidden_size\": 512,\n",
      "            \"has_position\": true,\n",
      "            \"has_hidden_layer\": true\n",
      "        },\n",
      "        \"get_transformer\": {\n",
      "            \"hidden_size\": 512,\n",
      "            \"num_heads\": 8,\n",
      "            \"d_ff\": 512,\n",
      "            \"dropout\": 0.2,\n",
      "            \"num_self_attns\": 4\n",
      "        },\n",
      "        \"encoder\": {\n",
      "            \"attn_encoder\": {\n",
      "                \"hidden_size\": 512,\n",
      "                \"memory_size\": 2,\n",
      "                \"num_heads\": 2,\n",
      "                \"dropout\": 0.2,\n",
      "                \"num_cross_attns\": 2\n",
      "            },\n",
      "            \"img_encoder\": {\n",
      "                \"img_feat_size\": 2048,\n",
      "                \"hidden_size\": 512,\n",
      "                \"dropout\": 0.2\n",
      "            },\n",
      "            \"text_encoder\": {\n",
      "                \"hist_encoder\": {\n",
      "                    \"type\": \"transformer\"\n",
      "                },\n",
      "                \"ques_encoder\": {\n",
      "                    \"type\": \"transformer\"\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"decoder\": {\n",
      "            \"disc\": {\n",
      "                \"opt_encoder\": {\n",
      "                    \"type\": \"transformer\"\n",
      "                }\n",
      "            },\n",
      "            \"gen\": {\n",
      "                \"dropout\": 0.2,\n",
      "                \"vocab_size\": 11322,\n",
      "                \"hidden_size\": 512,\n",
      "                \"num_lstm_layers\": 2\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"solver\": {\n",
      "        \"device\": \"cuda\",\n",
      "        \"gpu_ids\": [\n",
      "            0,\n",
      "            1\n",
      "        ],\n",
      "        \"num_epochs\": 20,\n",
      "        \"init_lr\": 0.0005,\n",
      "        \"lr_steps\": [\n",
      "            15\n",
      "        ],\n",
      "        \"training_splits\": \"train\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quanguet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from comet_ml import Experiment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from visdial.model import VisdialModel\n",
    "from visdial.loss import get_criterion\n",
    "from visdial.data.dataset import VisDialDataset\n",
    "from visdial.metrics import SparseGTMetrics, NDCG, Monitor\n",
    "from visdial.utils.checkpointing import CheckpointManager, load_checkpoint\n",
    "from visdial.utils import move_to_cuda\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#   INPUT ARGUMENTS AND CONFIG AND PARAMS\n",
    "# =============================================================================\n",
    "\n",
    "from configs.transformer_config import get_config\n",
    "\n",
    "config = get_config()\n",
    "print(json.dumps(config, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config['seed']\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1208/45238 [00:00<00:03, 12078.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45238/45238 [00:03<00:00, 13065.52it/s]\n",
      "  8%|▊         | 2624/34822 [00:00<00:02, 13081.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34822/34822 [00:02<00:00, 13091.94it/s]\n",
      "100%|██████████| 2064/2064 [00:00<00:00, 11009.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VisDialDataset(config, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1225/45238 [00:00<00:03, 12249.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45238/45238 [00:03<00:00, 12683.11it/s]\n",
      "  4%|▍         | 1313/34822 [00:00<00:02, 13128.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34822/34822 [00:02<00:00, 13218.18it/s]\n",
      "100%|██████████| 2064/2064 [00:00<00:00, 10542.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=config['dataset']['batch_size'],\n",
    "                              num_workers=config['dataset']['cpu_workers'],\n",
    "                              shuffle=True,)\n",
    "\n",
    "val_dataset = VisDialDataset(config, split='val')\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=2,\n",
    "                            num_workers=config['dataset']['cpu_workers'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.model import get_transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_transformer_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisdialModel(\n",
       "  (encoder): Encoder(\n",
       "    (text_encoder): TextEncoder(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (tok_embedding): Embedding(11322, 300, padding_idx=0)\n",
       "        (pos_embedding): PositionalEmbedding()\n",
       "        (linear): Linear(in_features=300, out_features=512, bias=True)\n",
       "      )\n",
       "      (hist_encoder): HistEncoder(\n",
       "        (encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "        (summary_linear): SummaryAttention(\n",
       "          (attn_linear): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ques_encoder): QuesEncoder(\n",
       "        (encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (img_encoder): ImageEncoder(\n",
       "      (img_linear): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.2)\n",
       "      )\n",
       "    )\n",
       "    (attn_encoder): CrossAttentionEncoder(\n",
       "      (cross_attn_encoder): Sequential(\n",
       "        (0): CrossAttentionLayer(\n",
       "          (attns): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.2)\n",
       "              (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (1): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.2)\n",
       "              (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (2): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.2)\n",
       "              (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norms): ModuleList(\n",
       "            (0): NormalSubLayer(\n",
       "              (linear): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace)\n",
       "                (2): Dropout(p=0.2)\n",
       "              )\n",
       "            )\n",
       "            (1): NormalSubLayer(\n",
       "              (linear): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace)\n",
       "                (2): Dropout(p=0.2)\n",
       "              )\n",
       "            )\n",
       "            (2): NormalSubLayer(\n",
       "              (linear): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace)\n",
       "                (2): Dropout(p=0.2)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttentionLayer(\n",
       "          (attns): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.2)\n",
       "              (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (1): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.2)\n",
       "              (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (2): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.2)\n",
       "              (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norms): ModuleList(\n",
       "            (0): NormalSubLayer(\n",
       "              (linear): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace)\n",
       "                (2): Dropout(p=0.2)\n",
       "              )\n",
       "            )\n",
       "            (1): NormalSubLayer(\n",
       "              (linear): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace)\n",
       "                (2): Dropout(p=0.2)\n",
       "              )\n",
       "            )\n",
       "            (2): NormalSubLayer(\n",
       "              (linear): Sequential(\n",
       "                (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace)\n",
       "                (2): Dropout(p=0.2)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (summaries): ModuleList(\n",
       "      (0): SummaryAttention(\n",
       "        (attn_linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SummaryAttention(\n",
       "        (attn_linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): SummaryAttention(\n",
       "        (attn_linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder_linear): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (text_embeddings): TextEmbeddings(\n",
       "      (tok_embedding): Embedding(11322, 300, padding_idx=0)\n",
       "      (pos_embedding): PositionalEmbedding()\n",
       "      (linear): Linear(in_features=300, out_features=512, bias=True)\n",
       "    )\n",
       "    (disc_decoder): DiscDecoder(\n",
       "      (opt_encoder): OptEncoder(\n",
       "        (encoder): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): EncoderLayer(\n",
       "              (self_attn): MultiHeadAttention(\n",
       "                (dropout): Dropout(p=0.2)\n",
       "                (attn_fn): Attention(\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (linears): ModuleList(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (ffn_layer): PositionwiseFeedForward(\n",
       "                (ffn): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace)\n",
       "                  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (3): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "              (sublayers): ModuleList(\n",
       "                (0): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "                (1): SublayerConnection(\n",
       "                  (norm): LayerNorm()\n",
       "                  (dropout): Dropout(p=0.2)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm()\n",
       "        )\n",
       "        (summary_linear): SummaryAttention(\n",
       "          (attn_linear): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gen_decoder): GenDecoder(\n",
       "      (ans_rnn): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "      (lstm_to_words): Linear(in_features=512, out_features=11322, bias=True)\n",
       "      (dropout): Dropout(p=0.2)\n",
       "      (logsoftmax): LogSoftmax()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Size([512, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "    num_params += torch.tensor(p.size()).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 tensor(4.5976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "1 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "2 0 tensor(4.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "3 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "4 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "5 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "6 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "7 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "8 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "9 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "10 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "11 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "12 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "13 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "14 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "15 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "16 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "17 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "18 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "19 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "20 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "21 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "22 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "23 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "24 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "25 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "26 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "27 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "28 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "29 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "30 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "31 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "32 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "33 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "34 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "35 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "36 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "37 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "38 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "39 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "40 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "41 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "42 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "43 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "44 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "45 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "46 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "47 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "48 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "49 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "50 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "51 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "52 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "53 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "54 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "55 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "56 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "57 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "58 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "59 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "60 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "61 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "62 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "63 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "64 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "65 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "66 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "67 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "68 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "69 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "70 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "71 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "72 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "73 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "74 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "75 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "76 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "77 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "78 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "79 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "80 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "81 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "82 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "83 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "84 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "85 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "86 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "87 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "88 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "89 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "90 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "91 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "92 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "93 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "94 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "95 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "96 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "97 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "98 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "99 0 tensor(4.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        batch = move_to_cuda(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch)\n",
    "        \n",
    "        opt_target = batch['ans_ind']\n",
    "        \n",
    "        opt_out = output['opt_scores']\n",
    "        opt_out = opt_out.view(-1, opt_out.size(-1))\n",
    "        \n",
    "        batch_loss = criterion(opt_out, opt_target.view(-1))\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        print(epoch, idx, batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 50, 40, 11, 95, 40, 84, 20, 53, 10],\n",
       "        [33, 31, 20, 56, 60, 75, 64, 90, 54, 52]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['ans_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(50, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(40, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(20, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(40, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(80, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(50, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(40, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(70, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(70, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(40, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(20, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(10, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(20, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(50, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(80, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(80, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(10, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor(70, device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(opt_out[i].argmax())\n",
    "    print(opt_out[i][opt_out[i].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
