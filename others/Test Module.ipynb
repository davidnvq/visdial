{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME_PATH /Users/quanguet\n",
      "DATA_PATH /Users/quanguet/datasets/visdial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quanguet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Word counts do not exist at /Users/quanguet/datasets/visdial/visdial_1.0_word_counts_train.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/Repos/visdial/debug.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisDialDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, split)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_add_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_is_add_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_return_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_is_return_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/dataset.py\u001b[0m in \u001b[0;36m_get_tokenizer\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path_json_word_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counts_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/vocabulary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, word_counts_path, min_count)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_counts_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             raise FileNotFoundError(\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0;34mf\"Word counts do not exist at {word_counts_path}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Word counts do not exist at /Users/quanguet/datasets/visdial/visdial_1.0_word_counts_train.json"
     ]
    }
   ],
   "source": [
    "run debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run visdial/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttentionEncoder(\n",
       "  (cross_attn_encoder): Sequential(\n",
       "    (0): CrossAttentionLayer(\n",
       "      (attns): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0): NormalSubLayer(\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.2)\n",
       "          )\n",
       "        )\n",
       "        (1): NormalSubLayer(\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.2)\n",
       "          )\n",
       "        )\n",
       "        (2): NormalSubLayer(\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.2)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttentionLayer(\n",
       "      (attns): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.2)\n",
       "          (x_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (y_proj_linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norms): ModuleList(\n",
       "        (0): NormalSubLayer(\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.2)\n",
       "          )\n",
       "        )\n",
       "        (1): NormalSubLayer(\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.2)\n",
       "          )\n",
       "        )\n",
       "        (2): NormalSubLayer(\n",
       "          (linear): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.2)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrossAttentionEncoder(**config['model']['encoder']['attn_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.2, 'vocab_size': 11322, 'hidden_size': 512, 'num_lstm_layers': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['model']['decoder']['gen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_decoder = GenDecoder(**config['model']['decoder']['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opt_scores': tensor([[0.0154, 0.0048, 0.0904,  ..., 0.0010, 0.0037, 0.0021],\n",
       "         [0.0033, 0.0017, 0.0130,  ..., 0.0087, 0.0053, 0.0036],\n",
       "         [0.0010, 0.0052, 0.0023,  ..., 0.0058, 0.0042, 0.0079],\n",
       "         ...,\n",
       "         [0.0047, 0.0027, 0.0008,  ..., 0.0162, 0.0053, 0.0117],\n",
       "         [0.0179, 0.0490, 0.0060,  ..., 0.0289, 0.0105, 0.0075],\n",
       "         [0.0499, 0.0029, 0.0071,  ..., 0.0090, 0.0036, 0.0176]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " 'ans_out_scores': tensor([[[ 2.7939e-02,  6.3135e-03,  1.5509e-02,  ...,  3.6114e-02,\n",
       "           -8.7340e-04,  1.3230e-02],\n",
       "          [ 1.8456e-02,  1.9656e-02,  4.3824e-02,  ...,  3.7375e-02,\n",
       "           -2.0961e-02,  1.3503e-02],\n",
       "          [ 1.6448e-02,  1.1020e-02,  5.2877e-02,  ...,  4.0569e-02,\n",
       "           -2.3199e-03,  2.1958e-02],\n",
       "          ...,\n",
       "          [-8.4986e-03,  1.1478e-02,  6.7604e-02,  ...,  3.7108e-02,\n",
       "            1.5856e-02,  4.7003e-03],\n",
       "          [ 7.9965e-04, -9.9637e-03,  7.2030e-02,  ...,  2.8747e-02,\n",
       "           -3.2646e-03, -4.2790e-03],\n",
       "          [ 2.5575e-02,  6.7336e-03,  7.8184e-02,  ...,  9.0839e-03,\n",
       "            1.5660e-02,  5.2850e-04]],\n",
       " \n",
       "         [[ 2.6538e-02,  1.6091e-02,  1.9345e-02,  ...,  4.4487e-02,\n",
       "           -1.8009e-02,  1.2232e-02],\n",
       "          [ 1.7472e-02,  1.7764e-03,  4.7438e-02,  ...,  3.9082e-02,\n",
       "           -6.1936e-03,  6.1114e-03],\n",
       "          [ 2.3903e-02,  1.0792e-02,  5.2041e-02,  ...,  4.0209e-02,\n",
       "           -7.4675e-03,  4.9795e-04],\n",
       "          ...,\n",
       "          [-1.5377e-02, -2.3248e-03,  6.2591e-02,  ..., -1.0257e-02,\n",
       "           -1.5973e-02, -1.9727e-02],\n",
       "          [ 4.0807e-03,  8.6965e-04,  4.5668e-02,  ...,  9.0162e-03,\n",
       "            5.7072e-03, -8.7843e-03],\n",
       "          [ 4.3728e-03,  1.8334e-02,  3.8594e-02,  ...,  2.1288e-03,\n",
       "           -2.4717e-02,  6.6230e-03]],\n",
       " \n",
       "         [[ 2.1195e-02, -4.1351e-05,  2.8175e-02,  ...,  4.1436e-02,\n",
       "           -1.9846e-02,  2.2486e-02],\n",
       "          [ 1.4446e-02,  1.8256e-03,  2.6796e-02,  ...,  4.9808e-02,\n",
       "           -2.5395e-02,  3.4708e-03],\n",
       "          [ 3.1128e-02,  1.4097e-02,  4.5197e-02,  ...,  4.5561e-02,\n",
       "           -2.9910e-02,  7.3017e-03],\n",
       "          ...,\n",
       "          [-3.8732e-03, -1.9141e-02,  8.4214e-02,  ...,  2.1609e-02,\n",
       "            6.7372e-03, -1.4070e-02],\n",
       "          [ 3.7104e-03,  7.4472e-04,  8.4046e-02,  ...,  2.0984e-02,\n",
       "           -9.5462e-03, -4.5294e-04],\n",
       "          [ 3.4521e-02,  2.6658e-02,  8.0467e-02,  ...,  1.7171e-02,\n",
       "           -6.4069e-03, -2.3683e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 3.6295e-02,  9.9662e-03,  3.5656e-02,  ...,  5.1494e-02,\n",
       "           -1.0192e-02,  2.9419e-02],\n",
       "          [ 2.9483e-02,  1.2975e-02,  5.0102e-02,  ...,  3.2102e-02,\n",
       "           -2.0484e-02,  2.3374e-02],\n",
       "          [ 2.6035e-02,  1.1286e-03,  4.9982e-02,  ...,  4.1418e-02,\n",
       "           -9.0238e-03,  1.4499e-02],\n",
       "          ...,\n",
       "          [-2.1099e-03, -5.8829e-03,  5.6270e-02,  ...,  1.6368e-02,\n",
       "            7.7277e-03,  2.3804e-04],\n",
       "          [ 2.8085e-02, -1.5310e-02,  6.2174e-02,  ...,  3.6646e-02,\n",
       "            9.1745e-03, -1.6345e-03],\n",
       "          [ 3.0428e-02,  1.1456e-02,  5.6868e-02,  ...,  1.0921e-02,\n",
       "           -2.1572e-02, -2.9279e-02]],\n",
       " \n",
       "         [[ 3.1995e-02,  1.5258e-02,  2.9433e-02,  ...,  4.5364e-02,\n",
       "           -8.9411e-03,  2.2823e-02],\n",
       "          [ 2.6726e-02,  1.7758e-02,  4.6282e-02,  ...,  4.5591e-02,\n",
       "           -1.0231e-02,  9.2373e-03],\n",
       "          [ 3.6965e-02, -3.6228e-03,  2.2849e-02,  ...,  6.2379e-02,\n",
       "           -1.9916e-04,  2.7939e-02],\n",
       "          ...,\n",
       "          [-1.3785e-04,  1.3479e-02,  7.3378e-02,  ..., -3.2995e-03,\n",
       "           -9.2418e-03, -1.3359e-02],\n",
       "          [ 1.7813e-02, -2.8246e-03,  6.0664e-02,  ...,  4.8266e-03,\n",
       "            2.6760e-02, -1.0998e-02],\n",
       "          [ 2.2065e-02, -2.0927e-03,  6.7243e-02,  ...,  2.7612e-02,\n",
       "           -7.6178e-03, -3.1094e-02]],\n",
       " \n",
       "         [[ 1.9931e-02,  1.0572e-02,  2.6199e-02,  ...,  4.0718e-02,\n",
       "           -3.1115e-03,  1.3332e-02],\n",
       "          [ 3.9165e-02,  1.8670e-02,  7.2632e-02,  ...,  5.0824e-02,\n",
       "           -1.7046e-02,  2.4165e-02],\n",
       "          [ 2.8846e-02,  5.0779e-03,  4.5271e-02,  ...,  5.0103e-02,\n",
       "           -1.0872e-02,  9.3001e-03],\n",
       "          ...,\n",
       "          [ 4.0040e-03,  2.0246e-02,  6.0217e-02,  ...,  1.7445e-02,\n",
       "            7.5545e-03,  6.4741e-03],\n",
       "          [ 2.4727e-03, -7.3098e-03,  6.9380e-02,  ..., -3.5090e-03,\n",
       "            3.6422e-02, -8.5479e-03],\n",
       "          [ 2.9155e-03,  9.5037e-03,  7.2416e-02,  ...,  2.2645e-02,\n",
       "            2.7781e-03, -1.3179e-02]]], grad_fn=<AddBackward0>),\n",
       " 'opts_out_scores': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
