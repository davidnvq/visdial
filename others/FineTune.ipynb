{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, OfflineExperiment\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from visdial.model import get_model\n",
    "from visdial.data.dataset import VisDialDataset\n",
    "from visdial.metrics import SparseGTMetrics, NDCG\n",
    "from visdial.utils.checkpointing import CheckpointManager, load_checkpoint_from_config\n",
    "from visdial.utils import move_to_cuda\n",
    "from options import get_comet_experiment, get_training_config_and_args\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from visdial.optim import Adam, LRScheduler, get_weight_decay_params\n",
    "from visdial.loss import FinetuneLoss\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_path = 'configs/v002_abc_LP_lkf_D36.yml'\n",
    "config = yaml.load(open(config_path),Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['dataset']['train_json_dense_dialog_path'] = '/media/local_workspace/quang/datasets/visdial/annotations/visdial_1.0_train_dense_sample.json'\n",
    "config['dataset']['finetune'] = True\n",
    "config['callbacks']['path_pretrained_ckpt'] = '/media/local_workspace/quang/checkpoints/visdial/CVPR/v002_abc_LP_lkf_D36/checkpoint_29.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing questions...\n",
      "[train] Tokenizing answers...\n",
      "[train] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VisDialDataset(config, split='train')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=config['solver']['batch_size'] * torch.cuda.device_count(),\n",
    "                              num_workers=config['solver']['cpu_workers'],\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n",
      "[val2018] Tokenizing answers...\n",
      "[val2018] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "val_dataset = VisDialDataset(config, split='val')\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=config['solver']['batch_size'] * torch.cuda.device_count(),\n",
    "                            num_workers=config['solver']['cpu_workers'],\n",
    "                            shuffle=True)\n",
    "\n",
    "eval_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=1 * torch.cuda.device_count(),\n",
    "                            num_workers=config['solver']['cpu_workers'],\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2064"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(config['callbacks']['path_pretrained_ckpt'])['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/quang/workspace/log/tensorboard/v002_abc_LP_lkf_D36'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['callbacks']['log_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTIMIZER\"\"\"\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "# =============================================================================\n",
    "#   SETUP BEFORE TRAINING LOOP\n",
    "# =============================================================================\n",
    "finetune_path = config['callbacks']['log_dir'] + '/finetune'\n",
    "os.makedirs(finetune_path)\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=finetune_path)\n",
    "checkpoint_manager = CheckpointManager(model, optimizer, finetune_path, config=config)\n",
    "\n",
    "sparse_metrics = SparseGTMetrics()\n",
    "disc_metrics = SparseGTMetrics()\n",
    "gen_metrics = SparseGTMetrics()\n",
    "ndcg = NDCG()\n",
    "disc_ndcg = NDCG()\n",
    "gen_ndcg = NDCG()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  events.out.tfevents.1570148078.local.29781.0\n"
     ]
    }
   ],
   "source": [
    "ls /home/quang/workspace/log/tensorboard/v002_abc_LP_lkf_D36/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"solver\"][\"training_splits\"] = \"trainval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if config[\"solver\"][\"training_splits\"] == \"trainval\":\n",
    "    iterations = (len(train_dataset) + len(val_dataset)) // (\n",
    "                torch.cuda.device_count() * config[\"solver\"][\"batch_size\"]) + 1\n",
    "    num_examples = torch.tensor(len(train_dataset) + len(val_dataset), dtype=torch.float)\n",
    "else:\n",
    "    iterations = len(train_dataset) // (config['solver']['batch_size'] * torch.cuda.device_count()) + 1\n",
    "    num_examples = torch.tensor(len(train_dataset), dtype=torch.float)\n",
    "\n",
    "global_iteration_step = start_epoch * iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "disc_criterion = FinetuneLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 254/255 [04:45<00:01,  1.11s/it, batch_loss=0.291, epoch=0]\n",
      "100%|██████████| 1032/1032 [07:00<00:00,  2.47it/s]\n",
      "100%|█████████▉| 254/255 [04:59<00:01,  1.22s/it, batch_loss=0.33, epoch=1] \n",
      "100%|██████████| 1032/1032 [07:20<00:00,  2.33it/s]\n",
      "100%|█████████▉| 254/255 [04:49<00:01,  1.12s/it, batch_loss=0.361, epoch=2]\n",
      "100%|██████████| 1032/1032 [07:09<00:00,  2.46it/s]\n",
      "100%|█████████▉| 254/255 [04:55<00:01,  1.18s/it, batch_loss=0.315, epoch=3]\n",
      "100%|██████████| 1032/1032 [07:07<00:00,  2.47it/s]\n",
      "100%|█████████▉| 254/255 [04:54<00:01,  1.17s/it, batch_loss=0.316, epoch=4]\n",
      "100%|██████████| 1032/1032 [07:07<00:00,  2.45it/s]\n",
      "100%|█████████▉| 254/255 [04:52<00:01,  1.25s/it, batch_loss=0.244, epoch=5]\n",
      "100%|██████████| 1032/1032 [07:09<00:00,  2.46it/s]\n",
      " 22%|██▏       | 55/255 [01:03<03:54,  1.17s/it, batch_loss=0.204, epoch=6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d7a8e957e752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opt_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0msparse_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opt_scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ans_ind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/repos/visdial/visdial/metrics.py\u001b[0m in \u001b[0;36mobserve\u001b[0;34m(self, predicted_scores, target_ranks)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# shape: (batch_size, num_rounds, num_options)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mpredicted_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_to_ranks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_ranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/repos/visdial/visdial/metrics.py\u001b[0m in \u001b[0;36mscores_to_ranks\u001b[0;34m(scores)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mranked_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;31m# convert from 0-99 ranks to 1-100 ranks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mranks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if config[\"solver\"][\"training_splits\"] == \"trainval\":\n",
    "    iterations = (len(train_dataset) + len(val_dataset)) // (\n",
    "                torch.cuda.device_count() * config[\"solver\"][\"batch_size\"]) + 1\n",
    "    num_examples = torch.tensor(len(train_dataset) + len(val_dataset), dtype=torch.float)\n",
    "else:\n",
    "    iterations = len(train_dataset) // (config['solver']['batch_size'] * torch.cuda.device_count()) + 1\n",
    "    num_examples = torch.tensor(len(train_dataset), dtype=torch.float)\n",
    "\n",
    "global_iteration_step = start_epoch * iterations\n",
    "\n",
    "for epoch in range(start_epoch, config['solver']['num_epochs']):\n",
    "    logging.info(f\"Training for epoch {epoch}:\")\n",
    "\n",
    "    with tqdm(total=iterations) as pbar:\n",
    "        if config[\"solver\"][\"training_splits\"] == \"trainval\":\n",
    "            combined_dataloader = itertools.chain(train_dataloader, val_dataloader)\n",
    "        else:\n",
    "            combined_dataloader = itertools.chain(train_dataloader)\n",
    "\n",
    "        epoch_loss = torch.tensor(0.0)\n",
    "        for i, batch in enumerate(combined_dataloader):\n",
    "            batch = move_to_cuda(batch, device)\n",
    "\n",
    "            # zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # do forward\n",
    "            out = model(batch)\n",
    "\n",
    "            # compute loss\n",
    "            batch_loss = torch.tensor(0.0, requires_grad=True, device='cuda')\n",
    "            if out.get('opt_scores') is not None:\n",
    "                scores = out['opt_scores']\n",
    "\n",
    "                sparse_metrics.observe(out['opt_scores'], batch['ans_ind'])\n",
    "                batch_loss = disc_criterion(scores, batch)\n",
    "\n",
    "            # compute gradients\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # update params\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(epoch=epoch,\n",
    "                             batch_loss=batch_loss.item())\n",
    "\n",
    "            # log metrics\n",
    "            summary_writer.add_scalar(f'train/batch_loss', batch_loss.item(), global_iteration_step)\n",
    "\n",
    "            global_iteration_step += 1\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            epoch_loss += batch[\"ans\"].size(0) * batch_loss.detach()\n",
    "\n",
    "    if out.get('opt_scores') is not None:\n",
    "        avg_metric_dict = {}\n",
    "        avg_metric_dict.update(sparse_metrics.retrieve(reset=True))\n",
    "\n",
    "        for metric_name, metric_value in avg_metric_dict.items():\n",
    "            logging.info(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "        summary_writer.add_scalars(f\"train/metrics\", avg_metric_dict, global_iteration_step)\n",
    "\n",
    "    epoch_loss /= num_examples\n",
    "    logging.info(f\"train/epoch_loss: {epoch_loss.item()}\\n\")\n",
    "    summary_writer.add_scalar(f'train/epoch_loss', epoch_loss.item(), global_iteration_step)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #   ON EPOCH END  (checkpointing and validation)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Validate and report automatic metrics.\n",
    "\n",
    "    if config['callbacks']['validate']:\n",
    "        # Switch dropout, batchnorm etc to the correct mode.\n",
    "        model.eval()\n",
    "\n",
    "        logging.info(f\"\\nValidation after epoch {epoch}:\")\n",
    "\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            move_to_cuda(batch, device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(batch)\n",
    "\n",
    "                if out.get('opt_scores') is not None:\n",
    "                    scores = out['opt_scores']\n",
    "                    disc_metrics.observe(scores, batch[\"ans_ind\"])\n",
    "\n",
    "                    if \"gt_relevance\" in batch:\n",
    "                        scores = scores[\n",
    "                                 torch.arange(scores.size(0)),\n",
    "                                 batch[\"round_id\"] - 1, :]\n",
    "\n",
    "                        disc_ndcg.observe(scores, batch[\"gt_relevance\"])\n",
    "\n",
    "                if out.get('opts_out_scores') is not None:\n",
    "                    scores = out['opts_out_scores']\n",
    "                    gen_metrics.observe(scores, batch[\"ans_ind\"])\n",
    "\n",
    "                    if \"gt_relevance\" in batch:\n",
    "                        scores = scores[\n",
    "                                 torch.arange(scores.size(0)),\n",
    "                                 batch[\"round_id\"] - 1, :]\n",
    "\n",
    "                        gen_ndcg.observe(scores, batch[\"gt_relevance\"])\n",
    "\n",
    "                if out.get('opt_scores') is not None and out.get('opts_out_scores') is not None:\n",
    "                    scores = (out['opts_out_scores'] + out['opt_scores']) / 2\n",
    "\n",
    "                    sparse_metrics.observe(scores, batch[\"ans_ind\"])\n",
    "                    if \"gt_relevance\" in batch:\n",
    "                        scores = scores[\n",
    "                                 torch.arange(scores.size(0)),\n",
    "                                 batch[\"round_id\"] - 1, :]\n",
    "\n",
    "                        ndcg.observe(scores, batch[\"gt_relevance\"])\n",
    "\n",
    "        avg_metric_dict = {}\n",
    "        avg_metric_dict.update(sparse_metrics.retrieve(reset=True, key='avg_'))\n",
    "        avg_metric_dict.update(ndcg.retrieve(reset=True, key='avg_'))\n",
    "\n",
    "        disc_metric_dict = {}\n",
    "        disc_metric_dict.update(disc_metrics.retrieve(reset=True, key='disc_'))\n",
    "        disc_metric_dict.update(disc_ndcg.retrieve(reset=True, key='disc_'))\n",
    "\n",
    "        gen_metric_dict = {}\n",
    "        gen_metric_dict.update(gen_metrics.retrieve(reset=True, key='gen_'))\n",
    "        gen_metric_dict.update(gen_ndcg.retrieve(reset=True, key='gen_'))\n",
    "\n",
    "        for metric_dict in [avg_metric_dict, disc_metric_dict, gen_metric_dict]:\n",
    "            for metric_name, metric_value in metric_dict.items():\n",
    "                logging.info(f\"{metric_name}: {metric_value}\")\n",
    "            summary_writer.add_scalars(f\"val/metrics\", metric_dict, global_iteration_step)\n",
    "\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1beef50b52ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "os.path.curdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/local_workspace/quang/checkpoints/visdial/CVPR/v002_abc_LP_lkf_D36'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(config['callbacks']['path_pretrained_ckpt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
