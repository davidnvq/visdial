{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "\n",
    "class SummaryAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(SummaryAttention, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "                )\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "class NormalSubLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "\n",
    "        super(NormalSubLayer, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(hidden_size*3, hidden_size),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Dropout(p=dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: shape [batch_size, M, hidden_size*3]\"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, memory_size=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.num_heads = num_heads\n",
    "        self.d_h = hidden_size // num_heads\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.x_proj_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.y_proj_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        self.x_memory = nn.Parameter(nn.init.xavier_uniform_(torch.empty(memory_size, hidden_size)))\n",
    "        self.y_memory = nn.Parameter(nn.init.xavier_uniform_(torch.empty(memory_size, hidden_size)))\n",
    "\n",
    "    def project(self, x, x_mem, linear):\n",
    "        x_mem_size = x.size(0), self.memory_size, self.hidden_size\n",
    "        x = torch.cat([x_mem.unsqueeze(0).expand(*x_mem_size), x], dim=1)\n",
    "        x_proj = linear(x)\n",
    "        x_proj = x.view(x.size(0), x.size(1), self.num_heads, self.d_h)\n",
    "        return x, x_proj\n",
    "\n",
    "\n",
    "    def forward(self, X, Y, mask_X, mask_Y):\n",
    "        \"\"\"\n",
    "        X: shape: [batch_size, M, hidden_size]\n",
    "        Y: shape: [batch_size, N, hidden_size]\n",
    "        mask_X: shape: [batch_size, M]\n",
    "        mask_Y: shape: [batch_size, N]\n",
    "        \"\"\"\n",
    "        memory_mask = X.new_ones((X.size(0), self.memory_size)).long()\n",
    "\n",
    "        mask_X = torch.cat([memory_mask, mask_X], dim=1)\n",
    "        mask_Y = torch.cat([memory_mask, mask_Y], dim=1)\n",
    "        M_mem, N_mem = mask_X.size(1), mask_Y.size(1)\n",
    "        \n",
    "        mask_X = mask_X[:, None, :, None].repeat(1, self.num_heads, 1, N_mem)\n",
    "        mask_Y = mask_Y[:, None, None, :].repeat(1, self.num_heads, M_mem, 1)\n",
    "\n",
    "        print('mask_X', mask_X.shape)\n",
    "        print('mask_Y', mask_Y.shape)\n",
    "        X_mem, X_proj = self.project(X, self.x_memory, self.x_proj_linear)\n",
    "        Y_mem, Y_proj = self.project(Y, self.y_memory, self.y_proj_linear)\n",
    "\n",
    "        # (1) shape [bs, num_heads, mem_size + M, d_h]\n",
    "        # (2) shape [bs, num_heads, d_h, mem_size + N]\n",
    "        X_proj = X_proj.permute(0, 2, 1, 3)\n",
    "        Y_proj = Y_proj.permute(0, 2, 3, 1)\n",
    "\n",
    "        # shape: [bs, num_heads, mem_size + M, mem_size + N]\n",
    "        affinity_matrix = torch.matmul(X_proj, Y_proj)\n",
    "        affinity_matrix = affinity_matrix.masked_fill(mask_X==0, -1e9)\n",
    "        affinity_matrix = affinity_matrix.masked_fill(mask_Y==0, -1e9)\n",
    "\n",
    "        attn_X_guided_by_Y = torch.softmax(affinity_matrix, dim=2)\n",
    "        attn_Y_guided_by_X = torch.softmax(affinity_matrix, dim=3)\n",
    "\n",
    "        # (1) shape [bs, mem_size + M, mem_size + N]\n",
    "        # (2) shape [bs, mem_size + M, mem_size + N]\n",
    "        attn_X_guided_by_Y = torch.mean(attn_X_guided_by_Y, dim=1)\n",
    "        attn_Y_guided_by_X = torch.mean(attn_Y_guided_by_X, dim=1)\n",
    "\n",
    "        # (1) shape: [bs, mem_size + N, hidden_size]\n",
    "        # (2) shape: [bs, mem_size + M, hidden_size]\n",
    "        X_attends_in_Y = torch.matmul(attn_X_guided_by_Y.transpose(1, 2), X_mem)\n",
    "        Y_attends_in_X = torch.matmul(attn_Y_guided_by_X, Y_mem)\n",
    "\n",
    "        X_attends_in_Y = X_attends_in_Y[:, self.memory_size:, :]\n",
    "        Y_attends_in_X = Y_attends_in_X[:, self.memory_size:, :]\n",
    "        return X_attends_in_Y, Y_attends_in_X\n",
    "\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, memory_size=1, dropout=0.0):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.mh_attentions  = clones(MultiHeadAttention(hidden_size, num_heads, memory_size, dropout), 3)\n",
    "        self.norm_sublayers = clones(NormalSubLayer(hidden_size, dropout), 3)\n",
    "\n",
    "\n",
    "    def forward(self, img, hist, ques, img_mask, hist_mask, ques_mask):\n",
    "        img_in_hist, hist_in_img = self.mh_attentions[0](img, hist, img_mask, hist_mask)\n",
    "        img_in_ques, ques_in_img = self.mh_attentions[1](img, ques, img_mask, ques_mask)\n",
    "        hist_in_ques, ques_in_hist = self.mh_attentions[2](hist, ques, hist_mask, ques_mask)\n",
    "\n",
    "        img  = self.norm_sublayers[0](torch.cat([img, hist_in_img, ques_in_img], dim=-1))\n",
    "        ques = self.norm_sublayers[1](torch.cat([ques, hist_in_ques, img_in_ques], dim=-1))\n",
    "        hist = self.norm_sublayers[2](torch.cat([hist, ques_in_hist, img_in_hist], dim=-1))\n",
    "        return img, hist, ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_X torch.Size([8, 2, 5, 7])\n",
      "mask_Y torch.Size([8, 2, 5, 7])\n",
      "mask_X torch.Size([8, 2, 5, 9])\n",
      "mask_Y torch.Size([8, 2, 5, 9])\n",
      "mask_X torch.Size([8, 2, 7, 9])\n",
      "mask_Y torch.Size([8, 2, 7, 9])\n",
      "torch.Size([8, 4, 20])\n",
      "torch.Size([8, 6, 20])\n",
      "torch.Size([8, 8, 20])\n"
     ]
    }
   ],
   "source": [
    "hs = 20\n",
    "num_heads = 2\n",
    "x = torch.randn(8, 4, 20)\n",
    "maskx = torch.randint(0, 2, size=(8, 4))\n",
    "\n",
    "y = torch.randn(8, 6, 20)\n",
    "masky = torch.randint(0, 2, size=(8, 6))\n",
    "\n",
    "z = torch.randn(8, 8, 20)\n",
    "maskz = torch.randint(0, 2, size=(8, 8))\n",
    "\n",
    "res = CrossAttentionLayer(hs, num_heads)(x, y, z, maskx, masky, maskz)\n",
    "for r in res:\n",
    "    print(r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run visdial/data/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1735/45238 [00:00<00:02, 17347.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45238/45238 [00:02<00:00, 17116.80it/s]\n",
      "  5%|▌         | 1786/34822 [00:00<00:01, 17852.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34822/34822 [00:01<00:00, 18265.59it/s]\n",
      " 57%|█████▋    | 1179/2064 [00:00<00:00, 4396.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2064/2064 [00:00<00:00, 6115.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_ids              torch.Size([])\n",
      "num_rounds           torch.Size([])\n",
      "opts                 torch.Size([10, 100, 25])\n",
      "opts_in              torch.Size([10, 100, 25])\n",
      "opts_out             torch.Size([10, 100, 25])\n",
      "opts_len             torch.Size([10, 100])\n",
      "ans                  torch.Size([10, 25])\n",
      "ans_in               torch.Size([10, 25])\n",
      "ans_out              torch.Size([10, 25])\n",
      "ans_len              torch.Size([10])\n",
      "ans_ind              torch.Size([10])\n",
      "gt_relevance         torch.Size([100])\n",
      "round_id             torch.Size([])\n",
      "img_feat             torch.Size([36, 2048])\n",
      "ques_feats           torch.Size([10, 23, 768])\n",
      "hist_feats           torch.Size([11, 768])\n",
      "ques_masks           torch.Size([10, 23])\n",
      "{\n",
      "    \"ROOT\": \"/home/ubuntu\",\n",
      "    \"seed\": 0,\n",
      "    \"dataset\": {\n",
      "        \"overfit\": true,\n",
      "        \"img_norm\": 1,\n",
      "        \"concat_history\": true,\n",
      "        \"batch_size\": 8,\n",
      "        \"cpu_workers\": 4,\n",
      "        \"max_seq_len\": 25,\n",
      "        \"path\": \"datasets/visdial\",\n",
      "        \"train\": {\n",
      "            \"path_feat_img\": \"features_faster_rcnn_x101_train.h5\",\n",
      "            \"path_json_dialogs\": \"visdial_1.0_train.json\",\n",
      "            \"path_feat_history\": \"features_bert_train_history.h5\",\n",
      "            \"path_feat_answers\": \"features_bert_train_answers.h5\",\n",
      "            \"path_feat_questions\": \"features_bert_train_questions.h5\",\n",
      "            \"path_json_dense_dialogs\": null\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"path_feat_img\": \"features_faster_rcnn_x101_val.h5\",\n",
      "            \"path_json_dialogs\": \"visdial_1.0_val.json\",\n",
      "            \"path_feat_history\": \"features_bert_val_history.h5\",\n",
      "            \"path_feat_answers\": \"features_bert_val_answers.h5\",\n",
      "            \"path_feat_questions\": \"features_bert_val_questions.h5\",\n",
      "            \"path_json_dense_dialogs\": \"visdial_1.0_val_dense_annotations.json\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"path_feat_img\": \"features_faster_rcnn_x101_test.h5\",\n",
      "            \"path_feat_history\": \"features_bert_test_dialogs.h5\",\n",
      "            \"path_feat_answers\": \"features_bert_test_answers.h5\",\n",
      "            \"path_feat_questions\": \"features_bert_test_questions.h5\",\n",
      "            \"path_json_dense_dialogs\": null\n",
      "        }\n",
      "    },\n",
      "    \"model\": {\n",
      "        \"hidden_size\": 768,\n",
      "        \"dropout\": 0.2,\n",
      "        \"tokenizer\": {\n",
      "            \"bert_model\": \"bert-base-uncased\",\n",
      "            \"do_lower_case\": true\n",
      "        },\n",
      "        \"encoder\": {\n",
      "            \"type\": \"lf\",\n",
      "            \"img_encoder\": {\n",
      "                \"img_feat_size\": 2048\n",
      "            },\n",
      "            \"txt_encoder\": {\n",
      "                \"type\": \"bert\",\n",
      "                \"vocab_size\": 30522,\n",
      "                \"embedding_size\": 768\n",
      "            }\n",
      "        },\n",
      "        \"decoder\": {\n",
      "            \"type\": \"disc\",\n",
      "            \"txt_encoder\": {\n",
      "                \"type\": \"lstm\",\n",
      "                \"vocab_size\": 30522,\n",
      "                \"embedding_size\": 768,\n",
      "                \"lstm_num_layers\": 2,\n",
      "                \"bidirectional\": false\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"callbacks\": {\n",
      "        \"validate\": true,\n",
      "        \"resume\": false,\n",
      "        \"path_pretrained_ckpt\": null,\n",
      "        \"path_dir_save_ckpt\": \"checkpoints/visdial/lf_disc/lf_bert_disc\",\n",
      "        \"comet_project\": \"test\"\n",
      "    },\n",
      "    \"solver\": {\n",
      "        \"device\": \"cuda\",\n",
      "        \"gpu_ids\": [\n",
      "            0\n",
      "        ],\n",
      "        \"num_epochs\": 50,\n",
      "        \"init_lr\": 0.0003,\n",
      "        \"lr_steps\": [\n",
      "            25,\n",
      "            50\n",
      "        ],\n",
      "        \"training_splits\": \"train\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "dataset, dataloader, batch = test_visdial_dataset(config)\n",
    "import json\n",
    "print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run visdial/encoders/txt_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run visdial/encoders/lf_encoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.utils import move_to_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = move_to_cuda(batch, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LFEncoder(config)\n",
    "encoder = encoder.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feats = encoder.img_embeddings(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_feats.size())\n",
    "print(img_feats.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_feats = encoder.txt_embeddings(batch, type='lf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in txt_feats:\n",
    "    print(key, txt_feats[key].size(), txt_feats[key].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = encoder(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run visdial/decoders/disc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DiscriminativeDecoder(config)\n",
    "decoder = decoder.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(10)\n",
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((10,))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Dropout(p=0.2).eval()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Dropout(p=0.5).eval()(torch.ones(4, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones_like(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=2, bias=True)\n",
       "  (1): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.Sequential(nn.Linear(1, 2), nn.ReLU())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.x_proj_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.y_proj_linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    \n",
    "    def forward(self, X, Y, mask_X, mask_Y):\n",
    "        \"\"\"\n",
    "        X: shape: [batch_size, M, hidden_size]\n",
    "        Y: shape: [batch_size, N, hidden_size]\n",
    "        mask_X: shape: [batch_size, M]\n",
    "        mask_Y: shape: [batch_size, N]\n",
    "        \"\"\"\n",
    "        d_h = self.hidden_size // self.num_heads\n",
    "        \n",
    "        X_proj = self.x_proj_linear(X)\n",
    "        Y_proj = self.y_proj_linear(Y)\n",
    "        \n",
    "        X_proj = X_proj.view(X.size(0), X.size(1), self.num_heads, d_h)\n",
    "        Y_proj = Y_proj.view(Y.size(0), Y.size(1), self.num_heads, d_h)\n",
    "        \n",
    "        # shape: [bs, num_heads, M, d_h]\n",
    "        X_proj = X_proj.permute(0, 2, 1, 3)\n",
    "        # shape: [bs, num_heads, d_h, N]\n",
    "        Y_proj = Y_proj.permute(0, 2, 3, 1) \n",
    "        \n",
    "        # shape: [bs, num_heads, M, N] \n",
    "        affinity_matrix = torch.matmul(X_proj, Y_proj)\n",
    "\n",
    "        mask_X = mask_X[:, None, :, None].repeat(1, self.num_heads, 1, Y.size(1))\n",
    "        mask_Y = mask_Y[:, None, None, :].repeat(1, self.num_heads, X.size(1), 1)\n",
    "        affinity_matrix[~mask_X] = -9999999.0\n",
    "        affinity_matrix[~mask_Y] = -9999999.0\n",
    "        print(affinity_matrix)\n",
    "        \n",
    "        attn_X_guided_by_Y = torch.softmax(affinity_matrix, dim=2)\n",
    "        attn_Y_guided_by_X = torch.softmax(affinity_matrix, dim=3)\n",
    "        \n",
    "        print(attn_X_guided_by_Y)\n",
    "        print(attn_Y_guided_by_X)\n",
    "        # (1) shape: [bs, M, N]\n",
    "        # (2) shape: [bs, M, N]\n",
    "        attn_X_guided_by_Y = torch.mean(attn_X_guided_by_Y, dim=1)\n",
    "        attn_Y_guided_by_X = torch.mean(attn_Y_guided_by_X, dim=1)\n",
    "        \n",
    "        # (1) shape: [bs, N, hidden_size]\n",
    "        # (2) shape: [bs, M, hidden_size]\n",
    "        X_attends_in_Y = torch.matmul(attn_X_guided_by_Y.transpose(1, 2), X)\n",
    "        Y_attends_in_X = torch.matmul(attn_Y_guided_by_X, Y)\n",
    "        \n",
    "        return X_attends_in_Y, Y_attends_in_X\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_size': 5, \n",
    "    'num_heads' : 1\n",
    "}\n",
    "\n",
    "multi_heads_attn = MultiHeadAttention(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (x_proj_linear): Linear(in_features=5, out_features=5, bias=False)\n",
       "  (y_proj_linear): Linear(in_features=5, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_heads_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_x = torch.randint(0, 2, size=(2, 3)).byte()\n",
    "mask_y = torch.randint(0, 2, size=(2, 4)).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "          [-1.0000e+07, -1.0000e+07, -1.0000e+07, -1.0000e+07],\n",
      "          [-3.6676e-01, -1.0000e+07, -1.0000e+07, -6.3033e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6020e-01,  1.8862e-01,  5.2731e-02, -1.0000e+07],\n",
      "          [ 1.0865e+00,  1.7230e+00, -4.2912e-01, -1.0000e+07],\n",
      "          [-6.1388e-01, -1.0570e-01,  2.7913e-01, -1.0000e+07]]]],\n",
      "       grad_fn=<IndexPutBackward>)\n",
      "tensor([[[[0.0000, 0.3333, 0.3333, 0.0000],\n",
      "          [0.0000, 0.3333, 0.3333, 0.0000],\n",
      "          [1.0000, 0.3333, 0.3333, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1955, 0.1566, 0.3482, 0.3333],\n",
      "          [0.6802, 0.7266, 0.2151, 0.3333],\n",
      "          [0.1242, 0.1167, 0.4367, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[[[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "          [0.4246, 0.0000, 0.0000, 0.5754]]],\n",
      "\n",
      "\n",
      "        [[[0.2736, 0.3878, 0.3386, 0.0000],\n",
      "          [0.3216, 0.6078, 0.0706, 0.0000],\n",
      "          [0.1959, 0.3256, 0.4785, 0.0000]]]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([2, 4, 5])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(2, 3, 5)\n",
    "Y = torch.randn(2, 4, 5)\n",
    "\n",
    "a, b = multi_heads_attn(X, Y, mask_x, mask_y)\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4033,  0.0483,  0.0527,  ..., -0.4376, -0.0761, -0.3726],\n",
       "         [ 0.6515, -0.7384,  0.0404,  ...,  0.0301, -0.1306, -0.5231],\n",
       "         [ 0.4033,  0.0483,  0.0527,  ..., -0.4376, -0.0761, -0.3726],\n",
       "         ...,\n",
       "         [ 0.1078, -0.1965, -0.4716,  ...,  0.0100, -0.3892, -0.3319],\n",
       "         [ 0.7828, -0.3128, -0.7584,  ..., -0.3495, -0.1553, -0.0919],\n",
       "         [ 0.4033,  0.0483,  0.0527,  ..., -0.4376, -0.0761, -0.3726]],\n",
       "\n",
       "        [[-0.6709, -0.6391,  0.0901,  ..., -0.9314, -0.6749,  0.3258],\n",
       "         [-0.9335, -0.4887, -0.4550,  ..., -1.0240, -0.3731,  0.3095],\n",
       "         [-0.3399,  0.0126,  0.0423,  ..., -0.1205,  0.2022,  0.0717],\n",
       "         ...,\n",
       "         [-0.6586,  0.2715, -0.0574,  ..., -0.2848,  0.2655, -0.2963],\n",
       "         [-0.3399,  0.0126,  0.0423,  ..., -0.1205,  0.2022,  0.0717],\n",
       "         [-0.3399,  0.0126,  0.0423,  ..., -0.1205,  0.2022,  0.0717]],\n",
       "\n",
       "        [[-0.0742,  0.0781,  0.2334,  ...,  0.0329, -0.1039, -0.3782],\n",
       "         [-0.0742,  0.0781,  0.2334,  ...,  0.0329, -0.1039, -0.3782],\n",
       "         [-0.0742,  0.0781,  0.2334,  ...,  0.0329, -0.1039, -0.3782],\n",
       "         ...,\n",
       "         [-0.0742,  0.0781,  0.2334,  ...,  0.0329, -0.1039, -0.3782],\n",
       "         [-0.0742,  0.0781,  0.2334,  ...,  0.0329, -0.1039, -0.3782],\n",
       "         [-0.0742,  0.0781,  0.2334,  ...,  0.0329, -0.1039, -0.3782]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.8733,  0.1330, -0.2017,  ...,  0.2107, -0.7615,  0.2361],\n",
       "         [-0.0979, -0.0260,  0.0851,  ..., -0.0701, -0.1080, -0.1541],\n",
       "         [-0.7844, -0.1945, -0.1619,  ..., -0.6180, -0.8605,  0.0801],\n",
       "         ...,\n",
       "         [-0.0979, -0.0260,  0.0851,  ..., -0.0701, -0.1080, -0.1541],\n",
       "         [ 0.1765,  0.4778,  0.0722,  ..., -0.5634, -0.0634,  0.0391],\n",
       "         [-0.5074,  0.1758,  0.2324,  ..., -0.4690, -0.4660, -0.4412]],\n",
       "\n",
       "        [[ 0.0904, -0.1512,  0.2100,  ..., -0.6276, -0.2097, -0.2846],\n",
       "         [-0.0836,  0.2281,  0.7931,  ...,  0.1065, -0.2038, -0.2388],\n",
       "         [-0.4095,  0.5802, -0.3356,  ..., -0.7503,  0.0737, -0.5147],\n",
       "         ...,\n",
       "         [ 0.1199,  0.1384,  0.1582,  ..., -0.1395,  0.2992,  0.0471],\n",
       "         [ 0.1199,  0.1384,  0.1582,  ..., -0.1395,  0.2992,  0.0471],\n",
       "         [ 0.0482,  0.1403, -0.2942,  ..., -0.3257,  0.1367, -0.1423]],\n",
       "\n",
       "        [[ 0.0280, -0.3965, -0.5698,  ...,  0.0607,  0.2342, -0.2780],\n",
       "         [-0.1680, -0.1004,  0.0691,  ...,  0.3445,  0.1286,  0.0220],\n",
       "         [ 1.1726, -0.2430, -0.0693,  ...,  0.4951,  0.6676, -0.5786],\n",
       "         ...,\n",
       "         [-0.0333, -0.1508, -1.0690,  ...,  0.2088,  0.3988, -0.0395],\n",
       "         [-0.1680, -0.1004,  0.0691,  ...,  0.3445,  0.1286,  0.0220],\n",
       "         [-0.1680, -0.1004,  0.0691,  ...,  0.3445,  0.1286,  0.0220]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0059,  1.7653,  0.3292, -0.4051],\n",
       "         [-0.6591,  2.1579, -0.2128,  1.1763],\n",
       "         [-0.8461,  0.5538, -0.9316, -1.5338]],\n",
       "\n",
       "        [[ 1.1550,  0.6238,  0.7421,  0.5108],\n",
       "         [-0.2003,  0.4859, -1.5054, -0.8756],\n",
       "         [-0.2574,  0.3262,  1.3166, -1.3152]],\n",
       "\n",
       "        [[-0.1423,  0.1998,  0.8951, -1.7911],\n",
       "         [ 0.1389,  0.8564, -0.4722,  0.6451],\n",
       "         [-1.1392,  0.0230,  0.4652,  1.3334]],\n",
       "\n",
       "        [[-0.9671, -0.1166,  0.9532,  0.0507],\n",
       "         [ 0.7592,  0.5865,  1.4701,  0.1961],\n",
       "         [ 0.2382, -0.3408,  1.3652, -0.0586]],\n",
       "\n",
       "        [[ 0.2561, -0.4663, -0.9957, -1.3115],\n",
       "         [-0.0206,  2.5574,  0.2176,  0.9995],\n",
       "         [ 0.1746, -0.7608,  0.7546,  0.7625]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.randn(5, 3, 4)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0, 0],\n",
      "         [1, 0, 0, 0],\n",
      "         [1, 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0]],\n",
      "\n",
      "        [[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "    [1, 0, 0], \n",
    "    [1, 1, 0], \n",
    "    [1, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0]\n",
    "]).byte()\n",
    "a = a[:, :, None]\n",
    "a = a.repeat(1, 1, 4)\n",
    "#print(a)\n",
    "b = torch.tensor([\n",
    "    [1, 0, 0, 0], \n",
    "    [1, 1, 0, 0], \n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 1, 1],\n",
    "    [1, 1, 1, 0]\n",
    "]).byte()\n",
    "b = b[:, None, :]\n",
    "b = b.repeat(1, 3, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.5747e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-9.7794e-01,  7.3029e-02,  0.0000e+00,  0.0000e+00],\n",
       "         [-1.0049e+00, -3.3992e-01,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 1.1446e+00,  4.7268e-01,  3.2832e-01,  0.0000e+00],\n",
       "         [-6.1762e-01, -1.6292e+00,  1.9820e+00,  0.0000e+00],\n",
       "         [-4.7139e-01,  5.3024e-01, -2.9554e-01,  0.0000e+00]],\n",
       "\n",
       "        [[ 1.1559e-01,  6.6762e-01, -5.9305e-01,  5.8589e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 4.1537e-01,  1.0254e+00, -4.0526e-01,  6.9145e-02]],\n",
       "\n",
       "        [[-1.2107e-01, -1.1034e-03, -5.6049e-01,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "ma = copy.deepcopy(matrix)\n",
    "ma[~a] = 0\n",
    "ma[~b] = 0\n",
    "ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.2010e-02, 3.3531e-04, 4.9832e-01, 2.5950e-01],\n",
       "        [1.1420e-01, 1.2335e-04, 4.9832e-01, 3.5119e-02],\n",
       "        [8.4379e-01, 9.9954e-01, 3.3577e-03, 7.0538e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 1., 6., 2.],\n",
       "        [6., 0., 6., 0.],\n",
       "        [8., 9., 1., 3.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "run configs/lf_disc_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 0,\n",
       " 'callbacks': {'validate': True,\n",
       "  'resume': False,\n",
       "  'comet_project': 'lf-bert-disc',\n",
       "  'path_pretrained_ckpt': '',\n",
       "  'path_dir_save_ckpt': '/home/quanguet/checkpoints/visdial/lf_disc/lf_bert_disc'},\n",
       " 'dataset': {'overfit': False,\n",
       "  'img_norm': 1,\n",
       "  'concat_history': True,\n",
       "  'batch_size': 16,\n",
       "  'cpu_workers': 4,\n",
       "  'max_seq_len': 25,\n",
       "  'is_return_options': True,\n",
       "  'is_add_boundaries': True,\n",
       "  'train': {'path_feat_img': '/home/quanguet/datasets/visdial/features_faster_rcnn_x101_train.h5',\n",
       "   'path_json_dialogs': '/home/quanguet/datasets/visdial/visdial_1.0_train.json',\n",
       "   'path_feat_history': '/home/quanguet/datasets/visdial/features_bert_train_history.h5',\n",
       "   'path_feat_answers': '/home/quanguet/datasets/visdial/features_bert_train_answers.h5',\n",
       "   'path_feat_questions': '/home/quanguet/datasets/visdial/features_bert_train_questions.h5',\n",
       "   'path_json_dense_dialogs': '/home/quanguet/datasets/visdial/visdial_1.0_word_counts_train.json',\n",
       "   'path_json_word_count': '/home/quanguet/datasets/visdial/'},\n",
       "  'val': {'path_feat_img': '/home/quanguet/datasets/visdial/features_faster_rcnn_x101_val.h5',\n",
       "   'path_json_dialogs': '/home/quanguet/datasets/visdial/visdial_1.0_val.json',\n",
       "   'path_feat_history': '/home/quanguet/datasets/visdial/',\n",
       "   'path_feat_answers': '/home/quanguet/datasets/visdial/',\n",
       "   'path_feat_questions': '/home/quanguet/datasets/visdial/',\n",
       "   'path_json_dense_dialogs': '/home/quanguet/datasets/visdial/visdial_1.0_val_dense_annotations.json'},\n",
       "  'test': {'path_feat_img': '/home/quanguet/datasets/visdial/features_faster_rcnn_x101_test.h5',\n",
       "   'path_feat_history': '/home/quanguet/datasets/visdial/features_bert_test_dialogs.h5',\n",
       "   'path_feat_answers': '/home/quanguet/datasets/visdial/features_bert_test_answers.h5',\n",
       "   'path_feat_questions': '/home/quanguet/datasets/visdial/features_bert_test_questions.h5',\n",
       "   'path_json_dense_dialogs': '/home/quanguet/datasets/visdial/'}},\n",
       " 'model': {'hidden_size': 768,\n",
       "  'dropout': 0.2,\n",
       "  'tokenizer': 'nlp',\n",
       "  'encoder': {'type': 'lf',\n",
       "   'img_embeddings': {'img_feat_size': 2048},\n",
       "   'txt_embeddings': {'type': 'bert',\n",
       "    'vocab_size': 30522,\n",
       "    'embedding_size': 768}},\n",
       "  'decoder': {'type': 'disc',\n",
       "   'txt_embeddings': {'type': 'lstm',\n",
       "    'vocab_size': 30522,\n",
       "    'embedding_size': 768,\n",
       "    'lstm_num_layers': 2,\n",
       "    'bidirectional': False}}},\n",
       " 'solver': {'device': 'cuda',\n",
       "  'gpu_ids': [0, 1],\n",
       "  'num_epochs': 20,\n",
       "  'init_lr': 0.0005,\n",
       "  'lr_steps': [15],\n",
       "  'training_splits': 'train'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/quanguet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "run visdial/data/readers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2495/45238 [00:00<00:03, 12408.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45238/45238 [00:03<00:00, 11814.86it/s]\n",
      "  4%|▎         | 1237/34822 [00:00<00:02, 12367.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34822/34822 [00:02<00:00, 13190.89it/s]\n",
      "100%|██████████| 2064/2064 [00:00<00:00, 11019.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = DialogsReader(config, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run visdial/data/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ROOT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-33c6a62f10f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisDialDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, split)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_feat_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_img_feat_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mann_feat_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ann_feat_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialogs_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDialogsReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialogs_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, split)\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0mvisdial_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisdial_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdial_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdial_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"questions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdial_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/readers.py\u001b[0m in \u001b[0;36m_path_join\u001b[0;34m(config, name)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDialogsReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \t\"\"\"\n\u001b[1;32m     43\u001b[0m         \u001b[0mA\u001b[0m \u001b[0msimple\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mVisDial\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;36m.0\u001b[0m \u001b[0mdialog\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mjson\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ROOT'"
     ]
    }
   ],
   "source": [
    "dataset = VisDialDataset(config, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ROOT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f489c6cebd8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdialreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDialogsReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, split)\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdial_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdial_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"questions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisdial_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/readers.py\u001b[0m in \u001b[0;36m_path_join\u001b[0;34m(config, name)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDialogsReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \t\"\"\"\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mA\u001b[0m \u001b[0msimple\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mVisDial\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;36m.0\u001b[0m \u001b[0mdialog\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mjson\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msame\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmentioned\u001b[0m \u001b[0mon\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mvisualdialog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ROOT'"
     ]
    }
   ],
   "source": [
    "dialreader = DialogsReader(config, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a33d65ff2243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisdial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisDialDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisDialDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, split)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_add_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_is_add_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_return_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_is_return_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Repos/visdial/visdial/data/dataset.py\u001b[0m in \u001b[0;36m_get_is_add_boundaries\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_is_add_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_add_boundaries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_is_return_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset'"
     ]
    }
   ],
   "source": [
    "from visdial.data.dataset import VisDialDataset\n",
    "\n",
    "dataset = VisDialDataset(config, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "split='val'\n",
    "path_json_dialogs = config['dataset'][split]['path_json_dialogs']\n",
    "\n",
    "with open(path_json_dialogs, \"r\") as visdial_file:\n",
    "    visdial_data = json.load(visdial_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45237"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visdial_data['data']['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val2018'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visdial_data['split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
