{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_yml = 'configs/test_sigmoid_model.yml'\n",
    "config = yaml.load(open(config_yml),Loader=yaml.SafeLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdial.model import get_model\n",
    "from visdial.data.dataset import VisDialDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n",
      "[val2018] Tokenizing answers...\n",
      "[val2018] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "dataset = VisDialDataset(config, split='val')\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisdialModel(\n",
       "  (encoder): Encoder(\n",
       "    (text_encoder): TextEncoder(\n",
       "      (text_embedding): Embedding(11322, 300, padding_idx=0)\n",
       "      (hist_encoder): HistEncoder(\n",
       "        (hist_linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (hist_lstm): DynamicRNN(\n",
       "          (rnn_module): LSTM(300, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (pos_embedding): PositionalEmbedding()\n",
       "      )\n",
       "      (ques_encoder): QuesEncoder(\n",
       "        (ques_linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (ques_lstm): DynamicRNN(\n",
       "          (rnn_module): LSTM(300, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (pos_embedding): PositionalEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (img_encoder): ImageEncoder(\n",
       "      (img_linear): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (img_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (text_embedding): Embedding(11322, 300, padding_idx=0)\n",
       "      (cls_linear): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (attr_linear): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (x1_embedding): Embedding(600, 512)\n",
       "      (y1_embedding): Embedding(600, 512)\n",
       "      (x2_embedding): Embedding(600, 512)\n",
       "      (y2_embedding): Embedding(600, 512)\n",
       "      (bbox_linear): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (attn_encoder): CrossAttentionEncoder(\n",
       "      (cross_attn_encoder): Sequential(\n",
       "        (0): CrossAttentionLayer(\n",
       "          (attns): ModuleList(\n",
       "            (0): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): MultiHeadAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (im_mlp): NormalSubLayer(\n",
       "            (linear): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (qe_mlp): NormalSubLayer(\n",
       "            (linear): Sequential(\n",
       "              (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (im_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (qe_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (summaries): ModuleList(\n",
       "      (0): SummaryAttention(\n",
       "        (attn_linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SummaryAttention(\n",
       "        (attn_linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder_linear): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): MiscDecoder(\n",
       "    (disc_decoder): DiscriminativeDecoder(\n",
       "      (text_embedding): Embedding(11322, 300, padding_idx=0)\n",
       "      (opt_lstm): DynamicRNN(\n",
       "        (rnn_module): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (option_linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (gen_decoder): GenerativeDecoder(\n",
       "      (text_embedding): Embedding(11322, 300, padding_idx=0)\n",
       "      (answer_lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.1)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (lstm_to_words): Linear(in_features=512, out_features=11322, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (logsoftmax): LogSoftmax()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(4, 10, 4 ,128, 1)\n",
    "y = torch.randn(4, 10, 4, 100,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 100, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(y, x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(z.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
